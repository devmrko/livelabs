#!/usr/bin/env python3
"""
LiveLabs Streamlit Application with REST API Integration
Uses FastAPI REST services instead of MCP protocol

LiveLabs AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ - Streamlit Ïõπ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò
REST API ÏÑúÎπÑÏä§Îì§Í≥º ÌÜµÌï©ÌïòÏó¨ ÏõåÌÅ¨ÏÉµ Í≤ÄÏÉâ, ÏûêÏó∞Ïñ¥ ÏøºÎ¶¨, ÏÇ¨Ïö©Ïûê Ïä§ÌÇ¨ Í¥ÄÎ¶¨ Í∏∞Îä• Ï†úÍ≥µ
"""

import streamlit as st
import requests
import json
import subprocess
import time
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from utils.genai_client import OracleGenAIClient

# Î°úÍπÖ ÏÑ§Ï†ï - Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Ïã§Ìñâ Í≥ºÏ†ï Ï∂îÏ†ÅÏö©
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Streamlit ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï - Ï†úÎ™©, ÏïÑÏù¥ÏΩò, Î†àÏù¥ÏïÑÏõÉ Íµ¨ÏÑ±
st.set_page_config(
    page_title="LiveLabs AI Assistant",
    page_icon="üî¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Ïª§Ïä§ÌÖÄ CSS - ÌÖçÏä§Ìä∏ Í∞ÄÎèÖÏÑ± Ìñ•ÏÉÅÏùÑ ÏúÑÌïú Ïä§ÌÉÄÏùºÎßÅ
st.markdown("""
<style>
    .stMarkdown {
        color: #262730 !important;
    }
    .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {
        color: #262730 !important;
    }
    .stMarkdown p {
        color: #262730 !important;
    }
    .element-container .stMarkdown {
        background-color: rgba(255, 255, 255, 0.8);
        padding: 10px;
        border-radius: 5px;
    }
</style>
""", unsafe_allow_html=True)

# REST API ÏÑúÎπÑÏä§ ÏÑ§Ï†ï - Í∞Å ÏÑúÎπÑÏä§Î≥Ñ ÏóîÎìúÌè¨Ïù∏Ìä∏, ÎèÑÍµ¨, ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§ Ï†ïÏùò
# SERVICES = {
#     "semantic_search": {
#         "name": "LiveLabs Semantic Search",
#         "description": "Find workshops by topic, technology, or content keywords. Use for: 'list workshops', 'find AI courses', 'show machine learning labs'. Does NOT consider user profiles.",
#         # ... rest of config
#     },
#     "nl_query": {
#         "name": "LiveLabs NL Query to Database", 
#         "description": "Get personalized recommendations based on user profiles and skills. Use when query mentions a specific person ('I am...', 'for Michael Chen') or asks about user skills/experience. Perfect for 'what should I learn?' questions.",
#         # ... rest of config
#     },
#     "user_skills_progression": {
#         "name": "LiveLabs User Skills Progression",
#         "description": "Update user skills or mark workshop completion. Use for: 'I completed workshop X', 'add Python skill', 'update my progress'. Only for data updates, not queries.",
SERVICES = {
    "semantic_search": {  # ÏãúÎß®Ìã± Í≤ÄÏÉâ ÏÑúÎπÑÏä§ - Î≤°ÌÑ∞ ÏûÑÎ≤†Îî© Í∏∞Î∞ò ÏõåÌÅ¨ÏÉµ Í≤ÄÏÉâ
        "name": "LiveLabs Semantic Search",
        "description": "Search LiveLabs workshops using semantic similarity and vector embeddings",
        "mcp_service": "livelabs-semantic-search-service",
        "base_url": "http://localhost:8001",  # Ìè¨Ìä∏ 8001ÏóêÏÑú Ïã§Ìñâ
        "use_when": ["workshop search", "find courses", "semantic search", "similar content"],
        "endpoints": {
            "search": "/search",
            "statistics": "/statistics",
            "health": "/health"
        },
        "tools": {
            "search_livelabs_workshops": {
                "description": "Search for LiveLabs workshops using semantic similarity based on query text",
                "parameters": {
                    "query": {"type": "string", "required": True, "description": "Search query text"},
                    "top_k": {"type": "integer", "required": False, "description": "Number of results to return (default: 10)"}
                },
                "use_when": "User wants to find workshops or courses related to specific topics",
                "examples": ["find machine learning workshops", "search for database courses", "show me cloud computing labs"]
            }
        }
    },
    "nl_to_sql": {  # ÏûêÏó∞Ïñ¥-SQL Î≥ÄÌôò ÏÑúÎπÑÏä§ - Oracle SELECT AI ÌôúÏö©
        "name": "Natural Language to SQL Query",
        "description": "Query database using natural language with Oracle SELECT AI",
        "mcp_service": "livelabs-nl-query-service",
        "base_url": "http://localhost:8002",  # Ìè¨Ìä∏ 8002ÏóêÏÑú Ïã§Ìñâ
        "use_when": ["natural language query", "database questions", "user data lookup", "skills inquiry"],
        "endpoints": {
            "query": "/users/search/nl",
            "health": "/health"
        },
        "tools": {
            "query_database_nl": {
                "description": "Query the database using natural language with Oracle SELECT AI",
                "parameters": {
                    "natural_language_query": {"type": "string", "required": True, "description": "Natural language query about users, skills, or workshops"}
                },
                "use_when": "User asks questions about data in natural language",
                "examples": ["Who are the Python developers?", "Show me users with cloud skills", "What workshops has John completed?"]
            }
        }
    },
    "skill_progression": {  # Ïä§ÌÇ¨ ÏßÑÌñâÎèÑ Í¥ÄÎ¶¨ ÏÑúÎπÑÏä§ - ÏÇ¨Ïö©Ïûê Ïä§ÌÇ¨ ÏóÖÎç∞Ïù¥Ìä∏ Î∞è ÏõåÌÅ¨ÏÉµ ÏôÑÎ£å Ï∂îÏ†Å
        "name": "User Skills and Workshop Progression",
        "description": "Update and manage user skills and workshop completion tracking",
        "mcp_service": "livelabs-user-progression-service", 
        "base_url": "http://localhost:8003",  # Ìè¨Ìä∏ 8003ÏóêÏÑú Ïã§Ìñâ
        "use_when": ["update skills", "track progress", "complete workshop", "skill management"],
        "endpoints": {
            "update_skills": "/skills/update",
            "complete_workshop": "/progression/update",
            "get_progress": "/progression/get",
            "add_skill": "/skills/update",
            "health": "/health"
        },
        "tools": {
            "update_user_skills": {
                "description": "Update a user's skills or add new skills to their profile",
                "parameters": {
                    "user_id": {"type": "string", "required": True, "description": "User ID to update"},
                    "skills": {"type": "array", "required": True, "description": "List of skills to add or update"},
                    "skill_level": {"type": "string", "required": False, "default": "beginner", "description": "Skill level: beginner, intermediate, advanced"}
                },
                "use_when": "User wants to add or update their skills",
                "examples": ["add Python to my skills", "I learned machine learning", "update my Java level to advanced"]
            },
            "mark_workshop_complete": {
                "description": "Mark a workshop as completed for a user",
                "parameters": {
                    "user_id": {"type": "string", "required": True, "description": "User ID"},
                    "workshop_id": {"type": "string", "required": True, "description": "Workshop ID or name"},
                    "completion_date": {"type": "string", "required": False, "description": "Completion date (ISO format)"}
                },
                "use_when": "User reports completing a workshop or course",
                "examples": ["I completed the Docker workshop", "finished AI fundamentals course", "mark Oracle DB course as done"]
            }
        }
    }
}

def log_step(step_name: str, message: str):
    """Ïã§Ìñâ Îã®Í≥Ñ Î°úÍπÖ - ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑÏôÄ Ìï®Íªò Streamlit ÏÇ¨Ïù¥ÎìúÎ∞îÏóê ÌëúÏãú"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    log_message = f"üîç {step_name}: {message}"
    logger.info(f"[{timestamp}] {log_message}")
    
    # Streamlit ÏÇ¨Ïù¥ÎìúÎ∞îÏóê Î°úÍ∑∏ Î©îÏãúÏßÄ ÌëúÏãú
    if "log_messages" not in st.session_state:
        st.session_state.log_messages = []
    
    st.session_state.log_messages.append(f"[{timestamp}] {log_message}")
    
    # ÏµúÍ∑º 50Í∞ú Î©îÏãúÏßÄÎßå Ïú†ÏßÄ (Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨)
    if len(st.session_state.log_messages) > 50:
        st.session_state.log_messages = st.session_state.log_messages[-50:]

class RESTServiceManager:
    """REST API ÏÑúÎπÑÏä§ Í¥ÄÎ¶¨Ïûê - ÏÑúÎπÑÏä§ ÏãúÏûë/Ï§ëÏßÄ/ÏÉÅÌÉú ÌôïÏù∏ Îã¥Îãπ"""
    
    def __init__(self):
        self.services = SERVICES  # ÏÑúÎπÑÏä§ ÏÑ§Ï†ï Ï†ïÎ≥¥
        self.processes = {}  # Ïã§Ìñâ Ï§ëÏù∏ ÌîÑÎ°úÏÑ∏Ïä§ Ï∂îÏ†Å
        log_step("RESTServiceManager", "REST ÏÑúÎπÑÏä§ Îß§ÎãàÏ†Ä Ï¥àÍ∏∞Ìôî")
    
    def start_service(self, service_key: str) -> bool:
        """REST API ÏÑúÎπÑÏä§ ÏãúÏûë"""
        service = self.services[service_key]
        
        try:
            log_step("StartService", f"{service['name']} MCP ÏÑúÎπÑÏä§ ÏãúÏûë: {service['mcp_service']}")
            
            # Ïù¥ÎØ∏ Ïã§Ìñâ Ï§ëÏù∏ÏßÄ ÌôïÏù∏
            if service_key in self.processes and self.processes[service_key].poll() is None:
                log_step("StartService", f"{service['name']} Ïù¥ÎØ∏ Ïã§Ìñâ Ï§ë")
                return True
            
            # ÏÑúÎπÑÏä§ ÌîÑÎ°úÏÑ∏Ïä§ ÏãúÏûë
            process = subprocess.Popen(
                ["python", service["script"]],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=os.getcwd()
            )
            
            self.processes[service_key] = process
            
            # ÏãúÏûë ÎåÄÍ∏∞ ÏãúÍ∞Ñ (ÏÑúÎπÑÏä§ Ï¥àÍ∏∞Ìôî)
            time.sleep(2)
            
            # ÏÑúÎπÑÏä§ ÏÉÅÌÉú ÌôïÏù∏
            if self.check_health(service_key):
                log_step("StartService", f"‚úÖ {service['name']} started successfully")
                return True
            else:
                log_step("StartService", f"‚ùå {service['name']} failed to start properly")
                return False
                
        except Exception as e:
            log_step("StartService", f"‚ùå Error starting {service['name']}: {e}")
            return False
    
    def stop_service(self, service_key: str) -> bool:
        """REST API ÏÑúÎπÑÏä§ Ï§ëÏßÄ"""
        service = self.services[service_key]
        
        try:
            if service_key not in self.processes:
                log_step("StopService", f"{service['name']} Ïã§Ìñâ Ï§ëÏù¥ÏßÄ ÏïäÏùå")
                return True
            
            process = self.processes[service_key]
            
            if process.poll() is not None:
                log_step("StopService", f"{service['name']} Ïù¥ÎØ∏ Ï§ëÏßÄÎê®")
                del self.processes[service_key]
                return True
            
            log_step("StopService", f"{service['name']} Ï§ëÏßÄ Ï§ë")
            
            # ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å (Ï†ïÏÉÅ Ï¢ÖÎ£å ÏãúÎèÑ)
            process.terminate()
            
            # Ï†ïÏÉÅ Ï¢ÖÎ£å ÎåÄÍ∏∞ (5Ï¥à)
            try:
                process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                # ÌïÑÏöîÏãú Í∞ïÏ†ú Ï¢ÖÎ£å
                process.kill()
                process.wait()
            
            del self.processes[service_key]
            log_step("StopService", f"‚úÖ {service['name']} Ï§ëÏßÄ ÏôÑÎ£å")
            return True
            
        except Exception as e:
            log_step("StopService", f"‚ùå {service['name']} Ï§ëÏßÄ Ïò§Î•ò: {e}")
            return False
    
    def check_health(self, service_key: str) -> bool:
        """ÏÑúÎπÑÏä§ ÏÉÅÌÉú ÌôïÏù∏ (Ìó¨Ïä§Ï≤¥ÌÅ¨)"""
        service = self.services[service_key]
        
        try:
            response = requests.get(f"{service['base_url']}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_service_status(self, service_key: str) -> str:
        """ÏÑúÎπÑÏä§ ÏÉÅÌÉú Ï°∞Ìöå (Ïã§ÌñâÏ§ë/ÏãúÏûëÏ§ë/Ï§ëÏßÄÎê®)"""
        if service_key in self.processes and self.processes[service_key].poll() is None:
            if self.check_health(service_key):
                return "running"  # Ïã§ÌñâÏ§ë
            else:
                return "starting"  # ÏãúÏûëÏ§ë
        else:
            return "stopped"  # Ï§ëÏßÄÎê®

class AIReasoner:
    """AI Ï∂îÎ°† ÏóîÏßÑ - OCI GenAIÎ•º ÏÇ¨Ïö©Ìïú ÎèÑÍµ¨ ÏÑ†ÌÉù Î∞è ÏøºÎ¶¨ Î∂ÑÏÑù"""
    
    def __init__(self):
        log_step("AIReasoner", "AI Ï∂îÎ°† ÏóîÏßÑ Ï¥àÍ∏∞Ìôî")
        self.genai_client = OracleGenAIClient()  # Oracle GenAI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
    
    def reason_about_query(self, user_query: str) -> Dict[str, Any]:
        """ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨ Î∂ÑÏÑù Î∞è Ï†ÅÏ†àÌïú ÏÑúÎπÑÏä§/Ïï°ÏÖò Í≤∞Ï†ï"""
        log_step("AIReasoner", f"ÏøºÎ¶¨ Î∂ÑÏÑù Ï§ë: '{user_query}'")
        
        # Í∞Å Îã®Í≥ÑÎ≥Ñ ÏÇ¨Ïö©Ìï† Î™®Îç∏ Ï†ïÏùò
        reasoning_model = "cohere.command-r-plus-08-2024"
        
        # 1Îã®Í≥Ñ: LLMÏóêÍ≤å Ìï¥Ïïº Ìï† ÏùºÏóê ÎåÄÌï¥ ÏÉùÍ∞ÅÌïòÎèÑÎ°ù ÏöîÏ≤≠
        thinking_prompt = self._create_thinking_prompt(user_query)
        thinking_result = self._ai_think_about_query(thinking_prompt, user_query, reasoning_model)
        
        # 2Îã®Í≥Ñ: Í∞úÏÑ†Îêú ÌîÑÎ°¨ÌîÑÌä∏Î°ú ÏÑúÎπÑÏä§ Í≤∞Ï†ï
        decision_prompt = self._create_reasoning_prompt(user_query)
        ai_analysis = self._ai_analyze_query(decision_prompt, user_query, reasoning_model)
        
        # Í≤∞Í≥º Í≤∞Ìï© Î∞è Î™®Îç∏ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        ai_analysis["thinking_process"] = thinking_result
        ai_analysis["models_used"] = {
            "reasoning_model": reasoning_model,
            "summarization_model": "meta.llama-3.1-405b-instruct"
        }
        
        # LLMÏù¥ ÏôÑÏ†ÑÌûà Ïã§Ìå®Ìïú Í≤ΩÏö∞ Í∏∞Î≥∏ Ìè¥Î∞± Ï†úÍ≥µ
        if ai_analysis.get("error") and not ai_analysis.get("service"):
            log_step("AIReasoner", "LLM ÏôÑÏ†Ñ Ïã§Ìå®, Í∏∞Î≥∏ Ìè¥Î∞± ÏÇ¨Ïö©")
            query_lower = user_query.lower()
            if any(word in query_lower for word in ["i am", "my name", "what should i"]):
                ai_analysis = {
                    "service": "livelabs-user-skills-progression-service",
                    "tool": "query_user_skills_progression", 
                    "parameters": {"query": user_query},
                    "reasoning": "Ìè¥Î∞±: Í∞úÏù∏ ÏøºÎ¶¨ Í∞êÏßÄÎê®",
                    "confidence": 0.5
                }
            else:
                ai_analysis = {
                    "service": "livelabs-semantic-search-service",
                    "tool": "search_livelabs_workshops",
                    "parameters": {"query": user_query}, 
                    "reasoning": "Ìè¥Î∞±: ÏùºÎ∞ò Í≤ÄÏÉâ",
                    "confidence": 0.5
                }
            ai_analysis["thinking_process"] = thinking_result
        
        log_step("AIReasoner", f"AI ÏÇ¨Í≥† Í≥ºÏ†ï: {thinking_result.get('thought_process', 'ÏÇ¨Í≥† Í≥ºÏ†ï ÏóÜÏùå')}")
        log_step("AIReasoner", f"AI ÏÑ†ÌÉù ÏÑúÎπÑÏä§: {ai_analysis.get('service')}, ÎèÑÍµ¨: {ai_analysis.get('tool')}")
        
        return ai_analysis
    
    def _apply_fallback_logic(self, user_query: str) -> Dict[str, Any]:
        """LLMÏù¥ ÏÑúÎπÑÏä§ ÏÑ†ÌÉùÏóê Ïã§Ìå®ÌñàÏùÑ Îïå Í∑úÏπô Í∏∞Î∞ò Ìè¥Î∞± Î°úÏßÅ Ï†ÅÏö©"""
        query_lower = user_query.lower()
        
        # ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûêÏã†Ïùò Ïù¥Î¶ÑÏùÑ Ïñ∏Í∏âÌïòÍ±∞ÎÇò Í∞úÏù∏Ï†ÅÏù∏ Ï°∞Ïñ∏ÏùÑ ÏöîÏ≤≠ÌïòÎäîÏßÄ ÌôïÏù∏
        personal_indicators = ["i am", "my name is", "what should i", "recommend for me", "help me"]
        has_personal_context = any(indicator in query_lower for indicator in personal_indicators)
        
        # ÏõåÌÅ¨ÏÉµ/ÌïôÏäµ Í¥ÄÎ†® ÌÇ§ÏõåÎìú ÌôïÏù∏
        workshop_keywords = ["workshop", "learn", "course", "training", "skill", "big data", "ai", "machine learning", "python", "java"]
        is_workshop_related = any(keyword in query_lower for keyword in workshop_keywords)
        
        # Í≤∞Ï†ï Î°úÏßÅ
        if has_personal_context and is_workshop_related:
            # Í∞úÏù∏ Ï∂îÏ≤ú - Î®ºÏ†Ä nl_queryÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÇ¨Ïö©Ïûê Ïª®ÌÖçÏä§Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞
            return {
                "service": "nl_query",
                "action": "search_nl", 
                "reasoning": "Ìè¥Î∞± Î°úÏßÅ: ÏÇ¨Ïö©ÏûêÍ∞Ä Í∞úÏù∏ Ï∂îÏ≤úÏùÑ ÏöîÏ≤≠Ìï®, nl_queryÎ°ú ÏÇ¨Ïö©Ïûê ÌîÑÎ°úÌïÑÍ≥º Ïä§ÌÇ¨ Ï∞æÍ∏∞",
                "confidence": 0.7,
                "parameters": {"natural_language_query": user_query},
                "requires_user_context": True
            }
        elif is_workshop_related:
            # ÏùºÎ∞ò ÏõåÌÅ¨ÏÉµ Í≤ÄÏÉâ - ÏãúÎß®Ìã± Í≤ÄÏÉâ ÏÇ¨Ïö©
            return {
                "service": "semantic_search",
                "action": "search",
                "reasoning": "Fallback logic: General workshop search query, using semantic search",
                "confidence": 0.7,
                "parameters": {"query": user_query},
                "requires_user_context": False
            }
        else:
            # Í∏∞ÌÉÄ Î™®Îì† ÏøºÎ¶¨Ïóê ÎåÄÌï¥ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏãúÎß®Ìã± Í≤ÄÏÉâ ÏÇ¨Ïö©
            return {
                "service": "semantic_search", 
                "action": "search",
                "reasoning": "Ìè¥Î∞± Î°úÏßÅ: ÏùºÎ∞ò ÏøºÎ¶¨Ïóê ÎåÄÌï¥ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏãúÎß®Ìã± Í≤ÄÏÉâ ÏÇ¨Ïö©",
                "confidence": 0.6,
                "parameters": {"query": user_query},
                "requires_user_context": False
            }
    
    def _extract_json_from_text(self, text: str) -> Dict[str, Any] | None:
        """ÏûÑÏùòÏùò ÌÖçÏä§Ìä∏ÏóêÏÑú ÏΩîÎìú ÌéúÏä§ÎÇò Ï§ëÍ¥ÑÌò∏ Í∑†ÌòïÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï≤´ Î≤àÏß∏ Ïú†Ìö®Ìïú JSON Í∞ùÏ≤¥ Ï∂îÏ∂ú"""
        import re
        # 1) JSON ÏΩîÎìú Î∏îÎ°ù ÏãúÎèÑ
        fence = re.search(r"```json\s*([\s\S]*?)\s*```", text, re.IGNORECASE)
        if fence:
            candidate = fence.group(1).strip()
            try:
                return json.loads(candidate)
            except Exception:
                pass
        # 2) Î™®Îì† ÏΩîÎìú Î∏îÎ°ù ÏãúÎèÑ
        fence_any = re.search(r"```\s*([\s\S]*?)\s*```", text)
        if fence_any:
            candidate = fence_any.group(1).strip()
            try:
                return json.loads(candidate)
            except Exception:
                pass
        # 3) Ï≤´ Î≤àÏß∏ Í∞ùÏ≤¥Ïóê ÎåÄÌïú Ï§ëÍ¥ÑÌò∏ Í∑†Ìòï ÎßûÏ∂îÍ∏∞
        start = text.find('{')
        if start != -1:
            brace_count = 0
            in_string = False
            esc = False
            for i in range(start, len(text)):
                ch = text[i]
                if in_string:
                    if esc:
                        esc = False
                    elif ch == '\\':
                        esc = True
                    elif ch == '"':
                        in_string = False
                else:
                    if ch == '"':
                        in_string = True
                    elif ch == '{':
                        brace_count += 1
                    elif ch == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            candidate = text[start:i+1]
                            try:
                                return json.loads(candidate)
                            except Exception:
                                break
        # 4) Î™®Îì† ÏãúÎèÑ Ïã§Ìå®
        return None
    
    def _create_reasoning_prompt(self, user_query: str) -> str:
        """SERVICES ÏÑ§Ï†ïÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï∂îÎ°† ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        services_desc = []
        for service_key, service_config in SERVICES.items():
            service_use_when = service_config.get("use_when", [])
            tool_list = []
            for tool_name, tool_info in service_config.get("tools", {}).items():
                tool_use_when = tool_info.get("use_when", "")
                tool_list.append(f"   - {tool_name}: {tool_info['description']} (Use when: {tool_use_when})")
            
            use_when_text = f" (Use when: {', '.join(service_use_when)})" if service_use_when else ""
            services_desc.append(f"{service_key}{use_when_text}:\n" + "\n".join(tool_list))
        
        services_text = "\n\n".join(services_desc)
        
        return f"""You are a service selector for LiveLabs workshop recommendations. 

AVAILABLE MCP SERVICES AND TOOLS:
{services_text}

USER QUERY: "{user_query}"

CRITICAL: For queries like "I am [Name], what should I learn for [topic]", you MUST include next_step.

DECISION RULES:
- Personalized queries with name ‚Üí ALWAYS 2 steps: nl_to_sql first, then semantic_search
- General queries ‚Üí single step with semantic_search
- Update/progression queries ‚Üí use skill_progression service

REQUIRED OUTPUT FORMAT (include next_step for personalized queries):
{{"service": "nl_to_sql", "tool": "query_database_nl", "parameters": {{"natural_language_query": "Get all data for user [Name] including skills, experience levels, and workshop completion history"}}, "reasoning": "Get complete user profile first - filter by user, then let AI analyze", "next_step": {{"service": "semantic_search", "tool": "search_livelabs_workshops", "parameters": {{"query": "[topic] workshops"}}, "reasoning": "Find workshops matching topic - AI will match difficulty to user's actual skill level from step 1"}}}}

For general queries (no next_step):
{{"service": "semantic_search", "tool": "search_livelabs_workshops", "parameters": {{"query": "search term"}}, "reasoning": "Direct search"}}

RESPOND WITH VALID JSON FOR: "{user_query}"
"""
    
    

    def _ai_analyze_query(self, prompt: str, user_query: str, model_name: str) -> Dict[str, Any]:
        """LLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏøºÎ¶¨ Î∂ÑÏÑù Î∞è ÏÑúÎπÑÏä§ ÏÑ†ÌÉù"""
        log_step("AIAnalyzeQuery", f"LLM Ìò∏Ï∂úÌïòÏó¨ Î∂ÑÏÑù: '{user_query}'")
        
        response = self.genai_client.chat_json(
            prompt=prompt,
            model_name=model_name,
            temperature=0.0,
            max_tokens=300
        )
        
        if response["success"]:
            log_step("AIAnalyzeQuery", f"JSON ÏùëÎãµ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∞õÏùå: {response['json']}")
            return response["json"]
        else:
            log_step("AIAnalyzeQuery", f"GenAI Ìò∏Ï∂ú Ïã§Ìå®: {response.get('error', 'Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò')}")
            return {"error": response.get("error", "genai_failed"), "raw": response.get("raw_text", "")}

    def _create_thinking_prompt(self, user_query: str) -> str:
        """LLMÏù¥ Ìï¥Ïïº Ìï† ÏùºÏóê ÎåÄÌï¥ ÏÉùÍ∞ÅÌïòÎèÑÎ°ù ÌïòÎäî ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""
        schema = (
            '{"query_type":"personal_recommendation|general_search|update_request","user_mentioned":"string|null","needs_user_context":true,'
            '"thought_process":"string","recommended_approach":"string"}'
        )
        return f"""
You are an AI assistant analyzing a user's query to understand what information you need to give the best possible answer.

User Query: "{user_query}"

Think deeply about this query and what you need to do:

1. What is the user asking for?
2. What information would help give a better answer?
3. What's your reasoning strategy?

OUTPUT RULES (MANDATORY):
- Return ONLY a single JSON object on one line. No markdown, no prose, no code fences.
- Use only double quotes for all keys and strings.
- Escape any internal double quotes as \".
- Keep fields short to avoid long texts.

Respond EXACTLY with this JSON schema (no extra fields):
{schema}
"""

    def _create_decision_prompt(self, user_query: str, thinking_result: Dict) -> str:
        """Create service selection prompt based on thinking analysis"""
        
        # Build service descriptions from SERVICES configuration
        services_info = []
        for service_key, service_config in SERVICES.items():
            endpoints_list = []
            for action, endpoint in service_config.get("endpoints", {}).items():
                endpoints_list.append(f"   - {action}: {endpoint}")
            
            service_info = f"""**{service_key}** ({service_config['name']}):
   Description: {service_config['description']}
   Port: {service_config['port']}
   Available actions:
{chr(10).join(endpoints_list)}"""
            services_info.append(service_info)
        
        services_description = "\n\n".join(services_info)
        decision_schema = (
            '{"service":"service_key","action":"endpoint_key","reasoning":"string","confidence":0.85,'
            '"parameters":{"key":"value"}}'
        )
        return f"""
Based on your thinking analysis, now select the appropriate service and action.

**User Query:** "{user_query}"

**Your Previous Thinking:** {thinking_result}

**Available Services:**

{services_description}

**Now decide:** Based on your thinking above, which service should you call first?

**Response Format:**
{decision_schema}
"""

    def _ai_think_about_query(self, prompt: str, user_query: str, model_name: str) -> Dict[str, Any]:
        """LLMÏù¥ Î®ºÏ†Ä Ìï¥Ïïº Ìï† ÏùºÏóê ÎåÄÌï¥ ÏÉùÍ∞ÅÌïòÎèÑÎ°ù Ìï®"""
        log_step("AIThink", f"ÏÇ¨Í≥† Î∂ÑÏÑù ÏßÑÌñâ Ï§ë: '{user_query}'")
        
        response = self.genai_client.chat_json(
            prompt=prompt,
            model_name=model_name, 
            temperature=0.0,
            max_tokens=300
        )
        
        if response["success"]:
            log_step("AIThink", f"ÏÇ¨Í≥† JSON ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∞õÏùå: {response['json']}")
            return response["json"]
        else:
            log_step("AIThink", f"ÏÇ¨Í≥† GenAI Ìò∏Ï∂ú Ïã§Ìå®: {response.get('error', 'Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò')}")
            return {"error": response.get("error", "thinking_genai_failed"), "raw": response.get("raw_text", "")}

def call_rest_api(service_key: str, action: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """ÏÑúÎπÑÏä§Ïóê REST API Ìò∏Ï∂ú ÏàòÌñâ"""
    service = SERVICES[service_key]
    
    try:
        log_step("CallRESTAPI", f"{service['name']}.{action} Ìò∏Ï∂ú, Îß§Í∞úÎ≥ÄÏàò: {params}")
        
        # ÎèÑÍµ¨ Ïù¥Î¶ÑÏùÑ ÏóîÎìúÌè¨Ïù∏Ìä∏ Ïù¥Î¶ÑÏúºÎ°ú Îß§Ìïë
        tool_to_endpoint_map = {
            "search_livelabs_workshops": "search",
            "query_database_nl": "query", 
            "update_user_skills": "update_skills",
            "mark_workshop_complete": "complete_workshop",
            "get_user_progress": "get_progress"
        }
        
        # Ïã§Ï†ú ÏóîÎìúÌè¨Ïù∏Ìä∏ Ïù¥Î¶Ñ Í∞ÄÏ†∏Ïò§Í∏∞
        endpoint_name = tool_to_endpoint_map.get(action, action)
        
        # ÏÑúÎπÑÏä§ ÏóîÎìúÌè¨Ïù∏Ìä∏ÏóêÏÑú URL Íµ¨ÏÑ±
        if endpoint_name in service.get("endpoints", {}):
            url = f"{service['base_url']}{service['endpoints'][endpoint_name]}"
        else:
            log_step("CallRESTAPI", f"‚ùå Ïïå Ïàò ÏóÜÎäî Ïï°ÏÖò: {action} ({endpoint_name}ÏúºÎ°ú Îß§ÌïëÎê®)")
            return {"success": False, "error": f"Ïïå Ïàò ÏóÜÎäî Ïï°ÏÖò: {action} for service {service['name']}"}
        
        log_step("CallRESTAPI", f"üåê ÏöîÏ≤≠ URL: {url}")
        
        # ÏöîÏ≤≠ ÏàòÌñâ
        response = None
        if params:
            log_step("CallRESTAPI", f"üì§ JSON ÌéòÏù¥Î°úÎìúÏôÄ Ìï®Íªò POST ÏöîÏ≤≠")
            try:
                response = requests.post(url, json=params, timeout=30)
                log_step("CallRESTAPI", f"üì• POST ÏùëÎãµ: HTTP {response.status_code}")
            except Exception as post_error:
                log_step("CallRESTAPI", f"‚ö†Ô∏è POST Ïã§Ìå®, GET ÏãúÎèÑ: {post_error}")
                try:
                    response = requests.get(url, params=params, timeout=30)
                    log_step("CallRESTAPI", f"üì• GET ÏùëÎãµ: HTTP {response.status_code}")
                except Exception as get_error:
                    log_step("CallRESTAPI", f"‚ùå POSTÏôÄ GET Î™®Îëê Ïã§Ìå®: {get_error}")
                    return {"success": False, "error": f"ÏöîÏ≤≠ Ïã§Ìå®: POST({post_error}), GET({get_error})"}
        else:
            log_step("CallRESTAPI", f"üì§ GET ÏöîÏ≤≠ (Îß§Í∞úÎ≥ÄÏàò ÏóÜÏùå)")
            response = requests.get(url, timeout=30)
            log_step("CallRESTAPI", f"üì• GET ÏùëÎãµ: HTTP {response.status_code}")
        
        # ÏùëÎãµ Ï≤òÎ¶¨
        if response.status_code == 200:
            try:
                result = response.json()
                log_step("CallRESTAPI", f"‚úÖ {service['name']}.{action} ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏôÑÎ£å")
                log_step("CallRESTAPI", f"üìä ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ ÌÇ§: {list(result.keys()) if isinstance(result, dict) else 'ÎîïÏÖîÎÑàÎ¶¨ ÏïÑÎãò'}")
                
                # ÏùëÎãµÏóê ÎåÄÌïú Ï£ºÏöî Ï†ïÎ≥¥ Î°úÍ∑∏
                if isinstance(result, dict):
                    if "success" in result:
                        log_step("CallRESTAPI", f"üéØ API ÏÑ±Í≥µ: {result.get('success')}")
                    if "results" in result:
                        log_step("CallRESTAPI", f"üìã Í≤∞Í≥º Í∞úÏàò: {len(result.get('results', []))}")
                    if "total_found" in result:
                        log_step("CallRESTAPI", f"üîç Ï¥ù Î∞úÍ≤¨: {result.get('total_found')}")
                    if "error" in result and result.get("error"):
                        log_step("CallRESTAPI", f"‚ö†Ô∏è API Ïò§Î•ò Î∞òÌôò: {result.get('error')}")
                
                # Î™ÖÏãúÏ†ÅÏù∏ ÏÑ±Í≥µ ÌïÑÎìúÍ∞Ä ÏóÜÏúºÎ©¥ ÏÑ±Í≥µÏúºÎ°ú ÌëúÏãú
                if "success" not in result:
                    result["success"] = True
                    log_step("CallRESTAPI", f"üîß ÏùëÎãµÏóê success=True Ï∂îÍ∞Ä")
                
                return result
            except json.JSONDecodeError as e:
                error_msg = f"ÏûòÎ™ªÎêú JSON ÏùëÎãµ: {e}"
                log_step("CallRESTAPI", f"‚ùå JSON ÎîîÏΩîÎìú Ïò§Î•ò: {error_msg}")
                log_step("CallRESTAPI", f"üìÑ ÏõêÏãú ÏùëÎãµ: {response.text[:500]}...")
                return {"success": False, "error": error_msg}
        else:
            error_msg = f"HTTP {response.status_code}: {response.text[:200]}..."
            log_step("CallRESTAPI", f"‚ùå {service['name']}.{action} Ïã§Ìå®: {error_msg}")
            return {"success": False, "error": error_msg}
        
    except Exception as e:
        error_msg = f"ÏöîÏ≤≠ Ïã§Ìå®: {str(e)}"
        log_step("CallRESTAPI", f"‚ùå {service['name']}.{action} Ïò§Î•ò: {error_msg}")
        return {"success": False, "error": error_msg}

def generate_chatbot_response(user_query: str) -> str:
    """AI Ï∂îÎ°†Í≥º REST API Ìò∏Ï∂úÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï±óÎ¥á ÏùëÎãµ ÏÉùÏÑ±"""
    log_step("ChatbotResponse", f"ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨ Ï≤òÎ¶¨ Ï§ë: '{user_query}'")
    
    # Create reasoning display container
    reasoning_container = st.container()
    
    with reasoning_container:
        st.markdown("### üß† AI Reasoning Process")
        
        # Step 1: Query Analysis
        with st.expander("Step 1: Query Analysis", expanded=True):
            st.markdown(f"**User Query:** `{user_query}`")
            st.markdown("**Analysis:** Sending query to LLM for intent analysis...")
    
    # Initialize AI reasoner
    if "ai_reasoner" not in st.session_state:
        st.session_state.ai_reasoner = AIReasoner()
    
    # Step 2: AI Analysis
    with reasoning_container:
        with st.expander("Step 2: AI Service Selection", expanded=True):
            st.markdown("**LLM Analysis:** Oracle GenAI analyzing query...")
            
            # Show the reasoning process
            with st.spinner("ü§ñ AI is analyzing your query..."):
                ai_analysis = st.session_state.ai_reasoner.reason_about_query(user_query)
            
            # Update with actual model info after getting results
            if ai_analysis.get('models_used'):
                reasoning_model = ai_analysis['models_used'].get('reasoning_model', 'Unknown')
                st.markdown(f"**Model Used:** `{reasoning_model}`")
            
            # Display detailed AI reasoning
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Confidence", f"{ai_analysis.get('confidence', 0.0):.2f}")
                st.markdown(f"**Selected Service:** `{ai_analysis.get('service')}`")
                st.markdown(f"**Selected Action:** `{ai_analysis.get('tool') or ai_analysis.get('action')}`")
            
            with col2:
                if ai_analysis.get('parameters'):
                    st.markdown("**Extracted Parameters:**")
                    st.json(ai_analysis.get('parameters', {}))
            
            st.markdown("**AI Reasoning:**")
            st.info(ai_analysis.get('reasoning', 'No reasoning provided'))
    
    # Step 3: Multi-Step Workflow Execution
    with reasoning_container:
        with st.expander("Step 3: Multi-Step Workflow Execution", expanded=True):
            st.markdown("**Executing multi-step workflow...**")
            
            # Execute the multi-step workflow and get all results
            response, final_result, ai_analysis = generate_chatbot_response_with_data(user_query)
            
            # Display each step
            if final_result.get("steps"):
                for step_info in final_result["steps"]:
                    step_num = step_info["step"]
                    service = step_info["service"]
                    action = step_info["action"]
                    result = step_info["result"]
                    
                    st.markdown(f"### Step {step_num}: {service}.{action}")
                    st.markdown(f"**Parameters:** `{step_info['parameters']}`")
                    
                    if result.get("success"):
                        st.success(f"‚úÖ Step {step_num} successful!")
                        if "results" in result:
                            st.markdown(f"**Results found:** {len(result.get('results', []))}")
                        if "total_found" in result:
                            st.markdown(f"**Total found:** {result.get('total_found')}")
                    else:
                        st.error(f"‚ùå Step {step_num} failed: {result.get('error', 'Unknown error')}")
                    
                    st.divider()
            
            # Show workflow summary
            st.markdown(f"**Workflow Type:** {final_result.get('workflow_type', 'unknown')}")
            st.markdown(f"**Total Steps:** {final_result.get('total_steps', 0)}")
            st.markdown(f"**Overall Success:** {'‚úÖ' if final_result.get('success') else '‚ùå'}")
            
            return response
    
    # Step 4: Response Formatting
    with reasoning_container:
        with st.expander("Step 4: Response Formatting", expanded=True):
            st.markdown("**LLM will format the response based on the API results...**")
            
            # Let LLM format the response
            st.markdown(f"**API Response Status:** {'‚úÖ Success' if result.get('success') else '‚ùå Error'}")
            st.markdown(f"**Raw API Response Keys:** `{list(result.keys()) if isinstance(result, dict) else 'Not a dict'}`")
            
            # Simple response - let the LLM in the final results handle formatting
            if result.get("success"):
                response = f"Query: {user_query}\n\nAPI Results:\n{json.dumps(result, indent=2)}"
            else:
                response = f"Error processing query: {user_query}\nError: {result.get('error', 'Unknown error')}"
    
    log_step("ChatbotResponse", "Response generated successfully")
    return response

def generate_chatbot_response_with_data(user_query: str) -> tuple[str, Dict[str, Any], Dict[str, Any]]:
    """Generate chatbot response and return the raw data for LLM formatting"""
    log_step("ChatbotResponse", f"Processing user query: '{user_query}'")
    
    # Create reasoning display container
    reasoning_container = st.container()
    
    with reasoning_container:
        st.markdown("### üß† AI Reasoning Process")
        
        # Step 1: Query Analysis
        with st.expander("Step 1: Query Analysis", expanded=True):
            st.markdown(f"**User Query:** `{user_query}`")
            st.markdown("**Analysis:** Sending query to LLM for intent analysis...")
    
    # Initialize AI reasoner
    if "ai_reasoner" not in st.session_state:
        st.session_state.ai_reasoner = AIReasoner()
    
    # Step 2: AI Analysis
    with reasoning_container:
        with st.expander("Step 2: AI Service Selection", expanded=True):
            st.markdown("**LLM Analysis:** Two-step reasoning with Oracle GenAI...")
            
            # Show the reasoning process
            with st.spinner("ü§ñ AI is doing detailed reasoning analysis..."):
                ai_analysis = st.session_state.ai_reasoner.reason_about_query(user_query)
            
            # Show model information dynamically
            if ai_analysis.get('models_used'):
                reasoning_model = ai_analysis['models_used'].get('reasoning_model', 'Unknown')
                summarization_model = ai_analysis['models_used'].get('summarization_model', 'Unknown')
                st.markdown(f"**Models Used:** Reasoning: `{reasoning_model}` | Summarization: `{summarization_model}`")
            
            # Debug: Show raw AI analysis
            st.markdown("**üîç Debug: Raw AI Analysis Result**")
            st.json(ai_analysis)
            
            # Display thinking process first
            if ai_analysis.get('thinking_process'):
                st.markdown("**üß† Step 2a: LLM's Thinking Process**")
                thinking = ai_analysis['thinking_process']
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Query Type", thinking.get('query_type', 'Unknown'))
                with col2:
                    st.metric("User Mentioned", thinking.get('user_mentioned', 'None') or 'None')
                with col3:
                    st.metric("Needs Context", "Yes" if thinking.get('needs_user_context') else "No")
                
                st.markdown("**LLM's Step-by-Step Thinking:**")
                st.info(thinking.get('thought_process', 'No thinking provided'))
                
                st.markdown("**LLM's Recommended Approach:**")
                st.success(thinking.get('recommended_approach', 'No approach specified'))
                
                st.divider()
            
            # Display final service selection
            st.markdown("**üéØ Step 2b: Final Service Selection**")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Confidence", f"{ai_analysis.get('confidence', 0.0):.2f}")
                st.markdown(f"**Selected Service:** `{ai_analysis.get('service')}`")
                st.markdown(f"**Selected Action:** `{ai_analysis.get('tool') or ai_analysis.get('action')}`")
            
            with col2:
                if ai_analysis.get('parameters'):
                    st.markdown("**Extracted Parameters:**")
                    st.json(ai_analysis.get('parameters', {}))
            
            st.markdown("**Service Selection Reasoning:**")
            st.info(ai_analysis.get('reasoning', 'No reasoning provided'))
    
    # Step 3: API Call Execution
    with reasoning_container:
        with st.expander("Step 3: API Call Execution", expanded=True):
            service_key = ai_analysis.get("service")
            action = ai_analysis.get("tool") or ai_analysis.get("action")
            parameters = ai_analysis.get("parameters", {})
            
            # Validate that AI reasoner provided valid service and action
            if not service_key or not action:
                st.error("‚ùå AI reasoner failed to identify a valid service and action for your query.")
                st.markdown("**Debug Info:**")
                st.markdown(f"- Service: `{service_key}`")
                st.markdown(f"- Action: `{action}`")
                st.markdown("**Suggestion:** Try rephrasing your question to be more specific about what you want to do.")
                return "I couldn't determine which service to use for your request. Please try rephrasing your question.", {}, ai_analysis
            
            # Validate service exists
            if service_key not in SERVICES:
                st.error(f"‚ùå Unknown service: `{service_key}`")
                st.markdown(f"**Available services:** {list(SERVICES.keys())}")
                return f"Unknown service: {service_key}", {}, ai_analysis
            
            st.markdown(f"**Making API Call:** `{service_key}.{action}`")
            st.markdown(f"**Parameters:** `{parameters}`")
            
    
    # Step 3: Validate and make API call
    if not service_key or not action:
        error_msg = "AI reasoner failed to identify a valid service and action"
        log_step("ChatbotResponse", f"‚ùå {error_msg}")
        return error_msg, {"success": False, "error": error_msg}, ai_analysis
    
    if service_key not in SERVICES:
        error_msg = f"Unknown service: {service_key}"
        log_step("ChatbotResponse", f"‚ùå {error_msg}")
        return error_msg, {"success": False, "error": error_msg}, ai_analysis
    
    # Execute multi-step workflow
    all_results = []
    current_step = ai_analysis
    step_count = 0
    max_steps = 3
    
    while current_step and step_count < max_steps:
        step_count += 1
        step_service = current_step.get("service")
        step_action = current_step.get("tool") or current_step.get("action")
        step_params = current_step.get("parameters", {})
        
        log_step("ChatbotResponse", f"Executing step {step_count}: {step_service}.{step_action}")
        
        # Make API call for this step
        step_result = call_rest_api(step_service, step_action, step_params)
        
        all_results.append({
            "step": step_count,
            "service": step_service,
            "action": step_action,
            "parameters": step_params,
            "result": step_result,
            "reasoning": current_step.get("reasoning", "")
        })
        
        # Debug: Show what AI actually returned
        log_step("ChatbotResponse", f"AI analysis keys: {list(current_step.keys())}")
        if "next_step" in current_step:
            log_step("ChatbotResponse", f"Next step found: {current_step['next_step']}")
        else:
            log_step("ChatbotResponse", f"No 'next_step' key in AI response")
        
        # Check for next step and enhance it with previous step results
        if step_result.get("success") and current_step.get("next_step"):
            next_step = current_step.get("next_step").copy()
            
            # If next step is semantic search, enhance query with comprehensive context
            if next_step.get("service") == "semantic_search":
                
                # Build comprehensive context from original user query + all previous step results
                context_parts = [f"Original user inquiry: {user_query}"]
                
                # Add information from all previous steps
                for prev_step in all_results:
                    step_result_data = prev_step["result"]
                    if step_result_data.get("success"):
                        # Add user profile information from NL-to-SQL steps
                        if prev_step["service"] == "nl_to_sql":
                            if step_result_data.get("explanation"):
                                context_parts.append(f"User profile: {step_result_data['explanation']}")
                            if step_result_data.get("users"):
                                for user in step_result_data["users"]:
                                    if user.get("skills"):
                                        context_parts.append(f"User skills: {', '.join(user['skills'][:5])}")
                                    if user.get("experience_level"):
                                        context_parts.append(f"Experience level: {user['experience_level']}")
                                    if user.get("completed_workshops"):
                                        context_parts.append(f"Completed workshops: {len(user['completed_workshops'])} workshops")
                        
                        # Add any other relevant step results
                        elif prev_step["service"] == "semantic_search" and step_result_data.get("results"):
                            context_parts.append(f"Previous search found {len(step_result_data['results'])} workshops")
                
                # Combine original query with comprehensive context
                original_query = next_step.get("parameters", {}).get("query", "")
                comprehensive_query = f"{original_query}. Context: {' | '.join(context_parts)}"
                
                next_step["parameters"]["query"] = comprehensive_query
                log_step("ChatbotResponse", f"Enhanced query with full context: {comprehensive_query[:200]}...")
            
            current_step = next_step
            log_step("ChatbotResponse", f"Executing next step: {current_step.get('service')}.{current_step.get('tool')}")
        else:
            if current_step.get("next_step"):
                log_step("ChatbotResponse", f"Step {step_count} failed, skipping next step")
            else:
                log_step("ChatbotResponse", f"No next step specified - workflow complete")
            break
    
    # Combine results
    final_result = {
        "success": all(step["result"].get("success", False) for step in all_results),
        "workflow_type": "multi_step" if len(all_results) > 1 else "single_step", 
        "total_steps": len(all_results),
        "steps": all_results
    }
    
    # Generate response with LLM formatting
    if final_result.get("success"):
        response = format_response_with_llm(user_query, final_result, ai_analysis)
    else:
        response = f"Error processing query: {user_query}\nSome steps failed"
    
    log_step("ChatbotResponse", f"Workflow completed: {len(all_results)} steps")
    return response, final_result, ai_analysis


def format_response_with_llm(user_query: str, api_result: Dict[str, Any], ai_analysis: Dict[str, Any]) -> str:
    """Use LLM to format the final response using comprehensive data from all workflow steps"""
    try:
        genai_client = OracleGenAIClient()

        # Build comprehensive context from all steps
        comprehensive_context = {
            "user_query": user_query,
            "ai_analysis": ai_analysis,
            "workflow_type": api_result.get("workflow_type", "unknown"),
            "total_steps": api_result.get("total_steps", 0),
            "all_step_data": [],
            "final_result": api_result
        }
        
        # Collect data from each step
        if api_result.get("steps"):
            for step_info in api_result["steps"]:
                step_data = {
                    "step_number": step_info.get("step"),
                    "service": step_info.get("service"),
                    "action": step_info.get("action"),
                    "parameters": step_info.get("parameters"),
                    "result": step_info.get("result"),
                    "success": step_info.get("result", {}).get("success", False)
                }
                comprehensive_context["all_step_data"].append(step_data)

        # Create formatting prompt with comprehensive context
        formatting_prompt = f"""
You are an AI assistant helping users with LiveLabs workshop recommendations and database queries.

**User's Original Query:** "{user_query}"

**AI Analysis & Reasoning:** {ai_analysis.get('reasoning', 'No reasoning provided')}

**Workflow Summary:**
- Type: {comprehensive_context['workflow_type']}
- Total Steps: {comprehensive_context['total_steps']}
- Overall Success: {api_result.get('success', False)}

**Complete Step-by-Step Data:**
{json.dumps(comprehensive_context['all_step_data'], indent=2)}

**Final Aggregated Results:**
{json.dumps(api_result, indent=2)}

**Your Task:**
Analyze ALL step data and create a comprehensive, personalized response. Use information from EVERY step:

**Step 1 Analysis (User Profile):**
- Extract user's current skills, experience levels, and completed workshops
- Identify knowledge gaps and learning progression

**Step 2 Analysis (Workshop Search):**
- Review all found workshops and their difficulty levels
- Match workshops to user's skill level from Step 1

**Integration Requirements:**
1. **SYNTHESIZE ALL STEPS** - Combine user profile data with workshop search results
2. **PERSONALIZED MATCHING** - Explain why each workshop fits the user's current level
3. **LEARNING PATH** - Show progression from user's current state to recommended workshops
4. **COMPREHENSIVE REASONING** - Use both user data AND workshop data to justify recommendations

**Response Format:**
1. **User Profile Summary** (from Step 1 data)
2. **Recommended Learning Path** (synthesized from both steps)
3. **Specific Workshop Recommendations** with personalized reasoning
4. **Next Steps** for the user's learning journey

**CRITICAL RULES:**
- **USE ALL STEP DATA**: Don't ignore any step's information
- **CONNECT THE DOTS**: Show how user profile leads to specific workshop recommendations
- **PERSONALIZED REASONING**: Explain why each recommendation fits this specific user
- **INCLUDE URLs**: Format as [Workshop Title](URL) when available
- **NO HALLUCINATION**: Only use actual data from the steps

**LANGUAGE REQUIREMENT:**
Respond in Korean (ÌïúÍµ≠Ïñ¥). Use natural, professional Korean language throughout the entire response.

Create a comprehensive response that uses ALL available step information:
"""

        response = genai_client.chat(
            prompt=formatting_prompt,
            model_name="meta.llama-4-scout-17b-16e-instruct",
            temperature=0.3,
            max_tokens=1000
        )
        
        if response["success"]:
            log_step("FormatResponse", f"LLM formatted response successfully")
            return response["text"]
        else:
            log_step("FormatResponse", f"LLM formatting failed: {response.get('error', 'Unknown error')}")
            # Fallback to simple formatting
            return f"**Results for:** {user_query}\n\n```json\n{json.dumps(api_result, indent=2)}\n```"

    except Exception as e:
        log_step("FormatResponse", f"LLM formatting failed: {str(e)}")
        # Fallback to simple formatting
        return f"**Results for:** {user_query}\n\n```json\n{json.dumps(api_result, indent=2)}\n```"

def main():
    """Main Streamlit application"""
    log_step("MainApp", "Starting main application")
    
    # Initialize session state
    if "rest_service_manager" not in st.session_state:
        st.session_state.rest_service_manager = RESTServiceManager()
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Main title
    st.title("üî¨ LiveLabs AI Assistant")
    st.markdown("*Powered by REST APIs and OCI Generative AI*")
    
    # Sidebar for service management
    with st.sidebar:
        st.header("üîß REST API Services")
        
        for service_key, service in SERVICES.items():
            st.subheader(f"üîå {service['name']}")
            st.markdown(f"*{service['description']}*")
            st.markdown(f"**MCP Service:** {service['mcp_service']}")
            
            # Service status
            status = st.session_state.rest_service_manager.get_service_status(service_key)
            if status == "running":
                st.success("‚úÖ Running")
            elif status == "starting":
                st.warning("üîÑ Starting...")
            else:
                st.error("‚ùå Stopped")
            
            # Control buttons
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("‚ñ∂Ô∏è Start", key=f"start_{service_key}"):
                    with st.spinner(f"Starting {service['name']}..."):
                        success = st.session_state.rest_service_manager.start_service(service_key)
                        if success:
                            st.success("Started!")
                        else:
                            st.error("Failed to start")
                        st.rerun()
            
            with col2:
                if st.button("‚èπÔ∏è Stop", key=f"stop_{service_key}"):
                    with st.spinner(f"Stopping {service['name']}..."):
                        success = st.session_state.rest_service_manager.stop_service(service_key)
                        if success:
                            st.success("Stopped!")
                        else:
                            st.error("Failed to stop")
                        st.rerun()
            
            with col3:
                if st.button("üîç Test", key=f"test_{service_key}"):
                    with st.spinner(f"Testing {service['name']}..."):
                        healthy = st.session_state.rest_service_manager.check_health(service_key)
                        if healthy:
                            st.success("Healthy!")
                        else:
                            st.error("Not responding")
            
            st.divider()
        
        # Quick actions
        st.header("‚ö° Quick Actions")
        
        if st.button("üöÄ Start All Services"):
            log_step("UserAction", "User clicked start all services")
            with st.spinner("Starting all services..."):
                for service_key in SERVICES.keys():
                    st.session_state.rest_service_manager.start_service(service_key)
                st.success("All services started!")
                st.rerun()
        
        if st.button("üõë Stop All Services"):
            log_step("UserAction", "User clicked stop all services")
            with st.spinner("Stopping all services..."):
                for service_key in SERVICES.keys():
                    st.session_state.rest_service_manager.stop_service(service_key)
                st.success("All services stopped!")
                st.rerun()
        
        # Logs section
        st.header("üìã Activity Logs")
        if "log_messages" in st.session_state and st.session_state.log_messages:
            log_container = st.container()
            with log_container:
                for log_msg in st.session_state.log_messages[-10:]:  # Show last 10
                    st.text(log_msg)
        else:
            st.text("No activity yet...")
    
    # Main chat interface
    st.header("üí¨ AI Chat Assistant")
    
    # Chat input
    user_input = st.chat_input("Ask me about LiveLabs workshops, users, or skills...")
    
    if user_input:
        log_step("UserInput", f"User asked: '{user_input}'")
        
        # Add user message to chat history
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        
        # Show detailed reasoning process
        st.markdown("---")
        st.markdown("## üß† AI Reasoning & Processing")
        
        # Execute multi-step workflow and show results
        response, api_result, ai_analysis = generate_chatbot_response_with_data(user_input)
        
        # Show multi-step workflow execution details
        if api_result and api_result.get("steps"):
            st.markdown("### üîÑ Multi-Step Workflow Execution")
            
            for step_info in api_result["steps"]:
                step_num = step_info["step"]
                service = step_info["service"]
                action = step_info["action"]
                result = step_info["result"]
                
                with st.expander(f"Step {step_num}: {service}.{action}", expanded=True):
                    st.markdown(f"**Service:** {service}")
                    st.markdown(f"**Action:** {action}")
                    st.markdown(f"**Parameters:** `{step_info['parameters']}`")
                    
                    if result.get("success"):
                        st.success(f"‚úÖ Step {step_num} completed successfully!")
                        
                        # Show specific results for each service type
                        if service == "nl_to_sql" and "users" in result:
                            st.markdown(f"**Users found:** {result.get('total_found', 0)}")
                            if result.get("sql_query"):
                                st.code(result["sql_query"], language="sql")
                        
                        elif service == "semantic_search" and "results" in result:
                            st.markdown(f"**Workshops found:** {len(result.get('results', []))}")
                            
                            # Show first few workshop results
                            workshops = result.get("results", [])[:3]  # Show first 3
                            for i, workshop in enumerate(workshops, 1):
                                st.markdown(f"**{i}. {workshop.get('title', 'Unknown Workshop')}**")
                                if workshop.get('url'):
                                    st.markdown(f"üîó [View Workshop]({workshop['url']})")
                                if workshop.get('description'):
                                    st.markdown(f"üìù {workshop['description'][:100]}...")
                                st.markdown("---")
                    else:
                        st.error(f"‚ùå Step {step_num} failed: {result.get('error', 'Unknown error')}")
            
            # Workflow summary
            st.markdown("### üìä Workflow Summary")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Steps", api_result.get("total_steps", 0))
            with col2:
                st.metric("Workflow Type", api_result.get("workflow_type", "unknown"))
            with col3:
                success_status = "‚úÖ Success" if api_result.get("success") else "‚ùå Failed"
                st.metric("Status", success_status)
        
        # Final Results Display
        st.markdown("## üìã Final Results")
        with st.container():
            # Let LLM format the final response
            if api_result and api_result.get("success"):
                with st.spinner("ü§ñ LLM is formatting the final response..."):
                    formatted_response = format_response_with_llm(user_input, api_result, ai_analysis)
                st.markdown(formatted_response)
                # Use the formatted response for chat history
                final_response = formatted_response
            else:
                error_msg = f"‚ùå Error: {api_result.get('error', 'Unknown error') if api_result else 'No results'}"
                st.error(error_msg)
                final_response = error_msg
        
        st.markdown("---")
        
        # Add the formatted assistant response to chat history
        st.session_state.chat_history.append({"role": "assistant", "content": final_response})
    
    # Display chat history
    for message in st.session_state.chat_history:
        if message["role"] == "user":
            with st.chat_message("user"):
                st.markdown(message["content"])
        else:
            with st.chat_message("assistant"):
                st.markdown(message["content"])
    
    # REST API Testing Section
    st.header("üß™ REST API Testing")
    
    with st.expander("Test REST APIs Directly"):
        test_service = st.selectbox("Select Service", list(SERVICES.keys()))
        
        if test_service == "semantic_search":
            st.subheader("Semantic Search")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Search Workshops**")
                search_query = st.text_input("Search Query", "big data workshops")
                top_k = st.number_input("Top K Results", 1, 20, 5)
                
                if st.button("üîç Search"):
                    result = call_rest_api("semantic_search", "search", {"query": search_query, "top_k": top_k})
                    st.json(result)
            
            with col2:
                st.write("**Get Statistics**")
                
                if st.button("üìä Get Stats"):
                    result = call_rest_api("semantic_search", "statistics", {})
                    st.json(result)
        
        elif test_service == "nl_to_sql":
            st.subheader("Natural Language Query to Database")
            
            st.write("**Ask questions in natural language about users, skills, and profiles:**")
            
            # Example queries
            example_queries = [
                "Who are the Python developers?",
                "Find users with advanced Oracle skills",
                "Show me data scientists",
                "Who has machine learning experience?",
                "List all users with their skills",
                "Find John Smith's profile",
                "Who are the most experienced developers?",
                "Show users with cloud computing skills"
            ]
            
            col1, col2 = st.columns([2, 1])
            
            # Initialize session state for the query
            if 'current_nl_query' not in st.session_state:
                st.session_state.current_nl_query = "Who are the Python developers?"
            
            with col1:
                nl_query = st.text_area("Natural Language Query", 
                                       value=st.session_state.current_nl_query,
                                       height=100,
                                       key="nl_query_textarea")
                
                if st.button("ü§ñ Execute NL Query"):
                    result = call_rest_api("nl_to_sql", "query_database_nl", {"natural_language_query": nl_query})
                    if result:
                        st.success("‚úÖ Query executed successfully!")
                        if result.get("explanation"):
                            st.markdown(result["explanation"])
                        else:
                            st.info("Query executed but returned no results (empty response)")
                        
                        st.write("**Technical Details:**")
                        st.code(result.get("sql_query", "No SQL query shown"))
                    else:
                        st.error("‚ùå Query failed")
            
            with col2:
                st.write("**Example Queries:**")
                for i, example in enumerate(example_queries, 1):
                    # Show first few words of the query in the button
                    button_text = example if len(example) <= 35 else example[:32] + "..."
                    if st.button(f"üìù {button_text}", key=f"example_{i}"):
                        st.session_state.current_nl_query = example
                        st.rerun()
        
        elif test_service == "skill_progression":
            st.subheader("User Skills Progression")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Update User Skills**")
                user_id = st.text_input("User ID", "USR001")
                skill_name = st.text_input("Skill Name", "Python")
                experience_level = st.selectbox("Experience Level", ["BEGINNER", "INTERMEDIATE", "ADVANCED"])
                
                if st.button("üìù Update Skills"):
                    params = {
                        "userId": user_id,
                        "skills": [{
                            "skillName": skill_name,
                            "experienceLevel": experience_level
                        }]
                    }
                    result = call_rest_api("skill_progression", "update_user_skills", params)
                    st.json(result)
            
            with col2:
                st.write("**Update Workshop Progression**")
                workshop_user_id = st.text_input("User ID for Workshop", "USR001", key="workshop_user")
                workshop_id = st.number_input("Workshop ID", 1, 2000, 979)
                status = st.selectbox("Status", ["STARTED", "COMPLETED"])
                rating = st.number_input("Rating (1-5)", 1, 5, 5) if status == "COMPLETED" else None
                
                if st.button("üìà Update Progression"):
                    progression = {
                        "userId": workshop_user_id,
                        "workshopId": workshop_id,
                        "status": status
                    }
                    if rating is not None:
                        progression["rating"] = rating
                    
                    params = {
                        "progressions": [progression]
                    }
                    result = call_rest_api("skill_progression", "mark_workshop_complete", params)
                    st.json(result)

if __name__ == "__main__":
    main()
