#!/usr/bin/env python3
"""
LiveLabs Streamlit Application with REST API Integration
Uses FastAPI REST services instead of MCP protocol

LiveLabs AI ì–´ì‹œìŠ¤í„´íŠ¸ - Streamlit ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
REST API ì„œë¹„ìŠ¤ë“¤ê³¼ í†µí•©í•˜ì—¬ ì›Œí¬ìƒµ ê²€ìƒ‰, ìì—°ì–´ ì¿¼ë¦¬, ì‚¬ìš©ì ìŠ¤í‚¬ ê´€ë¦¬ ê¸°ëŠ¥ ì œê³µ
"""

import streamlit as st
import requests
import json
import subprocess
import time
import logging
import os
from datetime import datetime
from typing import Dict, Any, List, Optional
from utils.genai_client import OracleGenAIClient
from utils.ai_reasoner import AIReasoner

# ë¡œê¹… ì„¤ì • - ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ê³¼ì • ì¶”ì ìš©
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Streamlit í˜ì´ì§€ ì„¤ì • - ì œëª©, ì•„ì´ì½˜, ë ˆì´ì•„ì›ƒ êµ¬ì„±
st.set_page_config(
    page_title="LiveLabs AI Assistant",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS - í…ìŠ¤íŠ¸ ê°€ë…ì„± í–¥ìƒì„ ìœ„í•œ ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .stMarkdown {
        color: #262730 !important;
    }
    .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {
        color: #262730 !important;
    }
    .stMarkdown p {
        color: #262730 !important;
    }
    .element-container .stMarkdown {
        background-color: rgba(255, 255, 255, 0.8);
        padding: 10px;
        border-radius: 5px;
    }
</style>
""", unsafe_allow_html=True)

def load_services_config() -> Dict[str, Any]:
    """ì„œë¹„ìŠ¤ ì„¤ì •ì„ JSON íŒŒì¼ì—ì„œ ë¡œë“œ"""
    config_path = os.path.join(os.path.dirname(__file__), 'config', 'services.json')
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        # Extract mcpServers from the new format
        services = config.get('mcpServers', {})
        logger.info(f"ì„œë¹„ìŠ¤ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {len(services)}ê°œ ì„œë¹„ìŠ¤")
        return services
    except FileNotFoundError:
        logger.error(f"ì„œë¹„ìŠ¤ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {config_path}")
        raise
    except json.JSONDecodeError as e:
        logger.error(f"ì„œë¹„ìŠ¤ ì„¤ì • íŒŒì¼ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        raise
    except Exception as e:
        logger.error(f"ì„œë¹„ìŠ¤ ì„¤ì • ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise

# ì„œë¹„ìŠ¤ ì„¤ì • ë¡œë“œ
SERVICES = load_services_config()

class MCPToolDiscovery:
    """MCP ì„œë¹„ìŠ¤ ë„êµ¬ ë™ì  ë°œê²¬ ë° ìºì‹± ê´€ë¦¬"""
    
    def __init__(self, services_config: Dict[str, Any]):
        self.services = services_config
        self.tools_cache = {}
        
    async def discover_tools(self, service_key: str) -> Dict[str, Any]:
        """íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ë„êµ¬ë¥¼ ë°œê²¬í•˜ê³  ìºì‹œ"""
        if service_key not in self.services:
            raise ValueError(f"Unknown service: {service_key}")
            
        service = self.services[service_key]
        
        # ìºì‹œëœ ë„êµ¬ê°€ ìˆê³  ìµœê·¼ ë°œê²¬í•œ ê²½ìš° ë°˜í™˜
        if (service.get('tools_cache') and 
            service.get('last_discovery') and 
            self._is_cache_valid(service['last_discovery'])):
            return service['tools_cache']
            
        # REST APIë¥¼ í†µí•´ ë„êµ¬ ë°œê²¬
        try:
            tools_url = f"{service['baseUrl']}{service['endpoints']['tools']}"
            response = requests.get(tools_url, timeout=10)
            response.raise_for_status()
            
            tools_data = response.json()
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            service['tools_cache'] = tools_data
            service['last_discovery'] = datetime.now().isoformat()
            
            logger.info(f"ë„êµ¬ ë°œê²¬ ì™„ë£Œ - {service_key}: {len(tools_data.get('tools', []))}ê°œ ë„êµ¬")
            return tools_data
            
        except requests.RequestException as e:
            logger.error(f"ë„êµ¬ ë°œê²¬ ì‹¤íŒ¨ - {service_key}: {e}")
            # ìºì‹œëœ ë„êµ¬ê°€ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼
            return service.get('tools_cache', {'tools': []})
            
    def _is_cache_valid(self, last_discovery: str, max_age_minutes: int = 30) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            last_time = datetime.fromisoformat(last_discovery)
            age = datetime.now() - last_time
            return age.total_seconds() < (max_age_minutes * 60)
        except:
            return False
            
    def enable_service(self, service_key: str) -> bool:
        """ì„œë¹„ìŠ¤ í™œì„±í™” ë° ë„êµ¬ ë°œê²¬"""
        if service_key not in self.services:
            return False
            
        self.services[service_key]['enabled'] = True
        # ë¹„ë™ê¸° ë„êµ¬ ë°œê²¬ì€ ë³„ë„ë¡œ í˜¸ì¶œ
        return True
        
    def disable_service(self, service_key: str) -> bool:
        """ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”"""
        if service_key not in self.services:
            return False
            
        self.services[service_key]['enabled'] = False
        return True
        
    def get_enabled_services(self) -> List[str]:
        """í™œì„±í™”ëœ ì„œë¹„ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        return [key for key, service in self.services.items() 
                if service.get('enabled', False)]
                
    def get_available_tools(self, service_key: str) -> List[Dict[str, Any]]:
        """ì„œë¹„ìŠ¤ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
        if service_key not in self.services:
            return []
            
        service = self.services[service_key]
        # Ensure tools_cache is not None before accessing it
        tools_cache = service.get('tools_cache')
        if not tools_cache:
            return []
        return tools_cache.get('tools', [])


def log_step(step_name: str, message: str):
    """ì‹¤í–‰ ë‹¨ê³„ ë¡œê¹… - íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ Streamlit ì‚¬ì´ë“œë°”ì— í‘œì‹œ"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    log_message = f"ğŸ” {step_name}: {message}"
    logger.info(f"[{timestamp}] {log_message}")
    
    # Streamlit ì‚¬ì´ë“œë°”ì— ë¡œê·¸ ë©”ì‹œì§€ í‘œì‹œ
    if "log_messages" not in st.session_state:
        st.session_state.log_messages = []
    
    st.session_state.log_messages.append(f"[{timestamp}] {log_message}")
    
    # ìµœê·¼ 50ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
    if len(st.session_state.log_messages) > 50:
        st.session_state.log_messages = st.session_state.log_messages[-50:]

class RESTServiceManager:
    """REST API ì„œë¹„ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤ - ì„œë¹„ìŠ¤ ìƒíƒœ í…ŒìŠ¤íŠ¸ ë° ê±´ê°• ìƒíƒœ í™•ì¸"""
    
    def __init__(self, services_config: Dict[str, Any]):
        self.services = services_config
        
    def test_service(self, service_key: str) -> Dict[str, Any]:
        """ê°œë³„ ì„œë¹„ìŠ¤ ê±´ê°• ìƒíƒœ í…ŒìŠ¤íŠ¸"""
        if service_key not in self.services:
            return {
                "success": False,
                "status": "error",
                "message": f"Unknown service: {service_key}",
                "response_time": 0
            }
            
        service = self.services[service_key]
        health_url = f"{service['baseUrl']}{service['endpoints']['health']}"
        
        try:
            start_time = time.time()
            response = requests.get(health_url, timeout=5)
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                return {
                    "success": True,
                    "status": "healthy",
                    "message": f"Service {service['name']} is running",
                    "response_time": round(response_time * 1000, 2),  # ms
                    "details": response.json() if response.content else {}
                }
            else:
                return {
                    "success": False,
                    "status": "unhealthy", 
                    "message": f"Service returned status {response.status_code}",
                    "response_time": round(response_time * 1000, 2)
                }
                
        except requests.RequestException as e:
            return {
                "success": False,
                "status": "error",
                "message": f"Connection failed: {str(e)}",
                "response_time": 0
            }
            
    def test_all_services(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ì„œë¹„ìŠ¤ ê±´ê°• ìƒíƒœ í…ŒìŠ¤íŠ¸"""
        results = {}
        for service_key in self.services.keys():
            results[service_key] = self.test_service(service_key)
        return results
        
    async def discover_service_tools(self, service_key: str) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ì˜ ë„êµ¬ ë°œê²¬"""
        return await mcp_discovery.discover_tools(service_key)


def generate_chatbot_response_with_data(user_query: str) -> tuple[str, Dict[str, Any], Dict[str, Any]]:
    """Generate chatbot response and return the raw data for LLM formatting"""
    log_step("ChatbotResponse", f"Processing user query: '{user_query}'")
    
    # Initialize AI reasoner
    if "ai_reasoner" not in st.session_state:
        st.session_state.ai_reasoner = AIReasoner(st.session_state.services_config)
    
    # Step 1: AI Analysis
    ai_analysis = st.session_state.ai_reasoner.reason_about_query(user_query)
    
    # Step 2: Validate and make API call
    service_key = ai_analysis.get("service")
    action = ai_analysis.get("tool") or ai_analysis.get("action")
    parameters = ai_analysis.get("parameters", {})
    
    if not service_key or not action:
        error_msg = "AI reasoner failed to identify a valid service and action from the user query."
        log_step("ChatbotResponse", f"âŒ {error_msg}")
        # Create a final result that shows the initial failure
        final_result = {
            "success": False,
            "error": error_msg,
            "workflow_type": "failed_initialization",
            "total_steps": 0,
            "steps": [{
                "step": 1,
                "service": "AI-Reasoner",
                "action": "Initial-Analysis",
                "parameters": {"user_query": user_query},
                "result": {"success": False, "error": error_msg},
                "reasoning": ai_analysis.get('thinking_process', 'N/A')
            }]
        }
        return error_msg, final_result, ai_analysis
    
    if service_key not in SERVICES:
        error_msg = f"Unknown service: {service_key}"
        log_step("ChatbotResponse", f"âŒ {error_msg}")
        return error_msg, {"success": False, "error": error_msg}, ai_analysis
    
    # Execute dynamic multi-step workflow
    all_results = []
    step_count = 0
    max_steps = 5
    
    # Start with initial AI analysis
    current_step = ai_analysis
    
    while current_step and step_count < max_steps:
        step_count += 1
        step_service = current_step.get("service")
        step_action = current_step.get("tool") or current_step.get("action")
        step_params = current_step.get("parameters", {})
        
        log_step("ChatbotResponse", f"Executing step {step_count}: {step_service}.{step_action}")
        
        # Make API call for this step
        step_result = call_rest_api(step_service, step_action, step_params)
        
        all_results.append({
            "step": step_count,
            "service": step_service,
            "action": step_action,
            "parameters": step_params,
            "result": step_result,
            "reasoning": current_step.get("reasoning", "")
        })
        
        # Check if workflow should continue based on dynamic reasoning
        if step_result.get("success"):
            # Check if current step indicates completion
            if current_step.get("workflow_complete", False):
                log_step("ChatbotResponse", f"Workflow marked complete after step {step_count}")
                break
                
            # If not complete, use AI reasoner to determine next step
            log_step("ChatbotResponse", f"Step {step_count} successful, checking for next step...")
            
            # Re-analyze with context of previous results to determine next action
            next_analysis = st.session_state.ai_reasoner.reason_about_query(user_query, all_results)
            
            # If reasoner says we're done, stop
            if next_analysis.get("workflow_complete", False):
                log_step("ChatbotResponse", f"AI reasoner determined workflow complete after {step_count} steps")
                break
                
            # If reasoner suggests a different service than what we just did, continue
            if (next_analysis.get("service") != step_service or 
                next_analysis.get("tool") != step_action):
                
                # Enhance semantic search queries with context from previous steps
                if next_analysis.get("service") == "livelabs-semantic-search":
                    context_parts = [f"Original user inquiry: {user_query}"]
                    
                    # Add information from all previous steps
                    for prev_step in all_results:
                        step_result_data = prev_step.get("result", {})
                        if not step_result_data.get("success"):
                            continue
                            
                        # Process all keys in the result data
                        for key, value in step_result_data.items():
                            # Skip special keys and empty values
                            if key in ["success", "explanation"] or not value:
                                continue
                                
                            # Handle list of objects (like users, workshops, etc.)
                            if isinstance(value, list) and value and isinstance(value[0], dict):
                                for item in value:
                                    if not isinstance(item, dict):
                                        continue
                                    # Add all fields from each item
                                    for field, field_value in item.items():
                                        if not field_value:
                                            continue
                                        if isinstance(field_value, list):
                                            if field == "skills" and len(field_value) > 5:
                                                field_value = field_value[:5]  # Limit skills to top 5
                                            context_parts.append(f"{key}.{field}: {', '.join(map(str, field_value))}")
                                        else:
                                            context_parts.append(f"{key}.{field}: {field_value}")
                            # Handle simple key-value pairs
                            elif not isinstance(value, (dict, list)):
                                context_parts.append(f"{key}: {value}")
                            # Handle nested dictionaries
                            elif isinstance(value, dict):
                                for sub_key, sub_value in value.items():
                                    if sub_value:
                                        context_parts.append(f"{key}.{sub_key}: {sub_value}")
                        
                        # Add explanation if it exists
                        if step_result_data.get("explanation"):
                            context_parts.append(f"{prev_step['service']} context: {step_result_data['explanation']}")
                    
                    # Enhance query with context
                    original_query = next_analysis.get("parameters", {}).get("query", user_query)
                    comprehensive_query = f"{original_query}. Context: {' | '.join(context_parts)}"
                    next_analysis["parameters"]["query"] = comprehensive_query
                    log_step("ChatbotResponse", f"Enhanced query with context: {comprehensive_query[:200]}...")
                
                current_step = next_analysis
                log_step("ChatbotResponse", f"Continuing to next step: {current_step.get('service')}.{current_step.get('tool')}")
            else:
                log_step("ChatbotResponse", f"No new action suggested - workflow complete")
                break
        else:
            log_step("ChatbotResponse", f"Step {step_count} failed - stopping workflow")
            break
    
    # Combine results
    success = all(step["result"].get("success", False) for step in all_results) if all_results else False
    final_result = {
        "success": success,
        "workflow_type": "multi_step" if len(all_results) > 1 else "single_step", 
        "total_steps": len(all_results),
        "steps": all_results
    }
    
    # Generate response with LLM formatting
    if final_result.get("success"):
        response = format_response_with_llm(user_query, final_result, ai_analysis)
    else:
        # Even on failure, provide a helpful error message and the steps that were attempted
        failed_step = next((s for s in all_results if not s["result"].get("success")), None)
        if failed_step:
            error_details = failed_step['result'].get('error', 'Unknown error')
            response = f"I encountered an error during the process. Step {failed_step['step']} ({failed_step['action']}) failed with the following error: {error_details}"
        else:
            response = f"I'm sorry, I was unable to complete your request for '{user_query}'. An unexpected error occurred."

    log_step("ChatbotResponse", f"Workflow completed: {len(all_results)} steps")
    return response, final_result, ai_analysis


def format_response_with_llm(user_query: str, api_result: Dict[str, Any], ai_analysis: Dict[str, Any]) -> str:
    """Use LLM to format the API response into a natural language response"""
    log_step("LLMFormatting", f"Formatting response for query: '{user_query}'")
    
    # Prepare data for LLM formatting
    workflow_type = api_result.get("workflow_type", "single_step")
    total_steps = api_result.get("total_steps", 1)
    steps = api_result.get("steps", [])
    
    # Extract information from all steps dynamically
    all_step_data = {}
    
    for step in steps:
        step_result = step.get("result", {})
        if not step_result.get("success"):
            continue
            
        service_name = step.get("service", "unknown")
        if service_name not in all_step_data:
            all_step_data[service_name] = []
            
        # Process all keys in the result
        step_data = {}
        for key, value in step_result.items():
            # Skip special keys and empty values
            if key in ["success", "explanation"] or not value:
                continue
                
            # Handle different value types
            if isinstance(value, list):
                # For lists, take first 5 items if it's a list of objects
                if value and isinstance(value[0], dict):
                    step_data[key] = value[:5]
                else:
                    step_data[key] = value
            else:
                step_data[key] = value
                
        # Add explanation if it exists
        if step_result.get("explanation"):
            step_data["explanation"] = step_result["explanation"]
            
        all_step_data[service_name].append(step_data)
    
    # Backward compatibility
    user_info = all_step_data.get("livelabs-nl-query", [{}])[0].get("users", [{}])[0] if all_step_data.get("livelabs-nl-query") else {}
    workshop_results = all_step_data.get("livelabs-semantic-search", [{}])[0].get("results", [])
    
    # Create context for LLM using the dynamic all_step_data structure
    context_parts = [f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}"]
    workshops = []
    
    # Process all services and their data
    for service_name, service_steps in all_step_data.items():
        if not service_steps:
            continue
            
        context_parts.append(f"\n{service_name.upper()} ê²°ê³¼:")
        
        for step_idx, step_data in enumerate(service_steps, 1):
            for key, value in step_data.items():
                if key == 'explanation':
                    context_parts.append(f"- ì„¤ëª…: {value}")
                elif key == 'results' and isinstance(value, list):
                    # Add raw workshop results to context
                    for item in value[:5]:  # Limit to top 5 workshops
                        if isinstance(item, dict):
                            context_parts.append(f"- WORKSHOP_RAW: {item}")
                elif isinstance(value, list):
                    if value and isinstance(value[0], dict):
                        for item in value[:3]:  # Limit to top 3 items
                            if 'name' in item:
                                context_parts.append(f"- {key}: {item.get('name')}")
                    elif value:
                        context_parts.append(f"- {key}: {', '.join(map(str, value[:5]))}")
                elif value and not isinstance(value, (dict, list)):
                    context_parts.append(f"- {key}: {value}")
    
    context = "\n".join(context_parts)
    
    # Use LLM to format response
    formatting_prompt = """You are a helpful AI assistant for Oracle LiveLabs workshop recommendations.

CONTEXT:
{context}

TASK: Create a natural, conversational response in Korean that:
1. Acknowledges the user's query
2. If user profile information is available, mention relevant skills/experience
3. For each WORKSHOP_RAW entry:
   - Extract and present the workshop details in this format:
     ### [title]
     - **ë‚œì´ë„**: [difficulty]
     - **ì†Œìš” ì‹œê°„**: [duration]
     - **ì¹´í…Œê³ ë¦¬**: [category]
     - **ìœ ì‚¬ë„ ì ìˆ˜**: [similarity]%
     - [ë°”ë¡œê°€ê¸°](url)
4. Order workshops by highest similarity score first
5. Keep the tone warm, professional, and encouraging
6. Include all URLs exactly as provided in the raw data
7. Format the response in clean Markdown

IMPORTANT:
- The raw workshop data is in JSON format after 'WORKSHOP_RAW:'
- Use the exact field names from the raw data
- Don't modify or interpret the values, just present them as-is
- If a field is missing, skip that line
- Make sure all URLs are clickable Markdown links
- Use only the information provided in the context
- Keep the response concise but informative
- Use Korean throughout the response

Example of raw data format:
- WORKSHOP_RAW: {{'title': '...', 'url': '...', 'difficulty': '...', ...}}

Example output format for each workshop:
### [Workshop Title]
- **ë‚œì´ë„**: [Difficulty]
- **ì†Œìš” ì‹œê°„**: [Duration]
- **ì¹´í…Œê³ ë¦¬**: [Category]
- **ìœ ì‚¬ë„ ì ìˆ˜**: [Similarity]%
- [ë°”ë¡œê°€ê¸°](URL)
"""
    
    try:
        genai_client = OracleGenAIClient()
        llm_response = genai_client.chat(
            prompt=formatting_prompt.format(context=context),
            model_name="meta.llama-4-scout-17b-16e-instruct",
            temperature=0.3,
            max_tokens=500
        )
        
        if llm_response.get("success"):
            formatted_response = llm_response["text"]
            log_step("LLMFormatting", "âœ… LLM formatting successful")
            return formatted_response
        else:
            log_step("LLMFormatting", f"âŒ LLM formatting failed: {llm_response.get('error')}")
    except Exception as e:
        log_step("LLMFormatting", f"âŒ LLM formatting error: {str(e)}")
    
    # Fallback to simple formatting
    fallback_response = f"Based on your query '{user_query}', "
    
    if user_info:
        fallback_response += f"I found your profile information. "
    
    if workshop_results:
        fallback_response += f"I found {len(workshop_results)} relevant workshops:\n\n"
        for i, workshop in enumerate(workshop_results[:3], 1):
            title = workshop.get("title", "Untitled Workshop")
            url = workshop.get("url", "#")
            score = workshop.get("score", 0)
            fallback_response += f"{i}. **[{title}]({url})** (Relevance: {score:.1f}/10)\n"
    else:
        fallback_response += "I couldn't find specific workshops matching your criteria."
    
    return fallback_response


# ===== REST API TESTING FUNCTIONS =====

def call_rest_api(service_key: str, action: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """ì„œë¹„ìŠ¤ì— REST API í˜¸ì¶œ ìˆ˜í–‰"""
    service = st.session_state.services_config[service_key]

    try:
        # ì„œë¹„ìŠ¤ë³„ ë§¤ê°œë³€ìˆ˜ ì´ë¦„ ë§¤í•‘ ì²˜ë¦¬
        if service_key == "livelabs-nl-query" and "query" in params:
            log_step("CallRESTAPI", "ë§¤ê°œë³€ìˆ˜ ì´ë¦„ ë§¤í•‘: 'query' -> 'natural_language_query'")
            params["natural_language_query"] = params.pop("query")

        log_step("CallRESTAPI", f"{service['name']}.{action} í˜¸ì¶œ, ë§¤ê°œë³€ìˆ˜: {params}")
        
        # ë„êµ¬ ì´ë¦„ì„ ì—”ë“œí¬ì¸íŠ¸ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
        tool_to_endpoint_map = {
            "search_livelabs_workshops": "search",
            "query_database_nl": "query", 
            "update_user_skills": "update_skills",
            "mark_workshop_complete": "complete_workshop",
            "get_user_progress": "get_progress"
        }
        
        # ì‹¤ì œ ì—”ë“œí¬ì¸íŠ¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        endpoint_name = tool_to_endpoint_map.get(action, action)
        
        # ì„œë¹„ìŠ¤ ì—”ë“œí¬ì¸íŠ¸ì—ì„œ URL êµ¬ì„±
        if endpoint_name in service.get("endpoints", {}):
            url = f"{service['baseUrl']}{service['endpoints'][endpoint_name]}"
        else:
            log_step("CallRESTAPI", f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì•¡ì…˜: {action} ({endpoint_name}ìœ¼ë¡œ ë§¤í•‘ë¨)")
            return {"success": False, "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ì•¡ì…˜: {action} for service {service['name']}"}
        
        log_step("CallRESTAPI", f"ğŸŒ ìš”ì²­ URL: {url}")
        
        # ìš”ì²­ ìˆ˜í–‰
        response = None
        if params:
            log_step("CallRESTAPI", f"ğŸ“¤ JSON í˜ì´ë¡œë“œì™€ í•¨ê»˜ POST ìš”ì²­")
            try:
                response = requests.post(url, json=params, timeout=30)
                log_step("CallRESTAPI", f"ğŸ“¥ POST ì‘ë‹µ: HTTP {response.status_code}")
            except Exception as post_error:
                log_step("CallRESTAPI", f"âš ï¸ POST ì‹¤íŒ¨, GET ì‹œë„: {post_error}")
                try:
                    response = requests.get(url, params=params, timeout=30)
                    log_step("CallRESTAPI", f"ğŸ“¥ GET ì‘ë‹µ: HTTP {response.status_code}")
                except Exception as get_error:
                    log_step("CallRESTAPI", f"âŒ POSTì™€ GET ëª¨ë‘ ì‹¤íŒ¨: {get_error}")
                    return {"success": False, "error": f"ìš”ì²­ ì‹¤íŒ¨: POST({post_error}), GET({get_error})"}
        else:
            log_step("CallRESTAPI", f"ğŸ“¤ GET ìš”ì²­ (ë§¤ê°œë³€ìˆ˜ ì—†ìŒ)")
            response = requests.get(url, timeout=30)
            log_step("CallRESTAPI", f"ğŸ“¥ GET ì‘ë‹µ: HTTP {response.status_code}")
        
        # ì‘ë‹µ ì²˜ë¦¬
        if response.status_code == 200:
            try:
                result = response.json()
                log_step("CallRESTAPI", f"âœ… {service['name']}.{action} ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
                log_step("CallRESTAPI", f"ğŸ“Š ì‘ë‹µ ë°ì´í„° í‚¤: {list(result.keys()) if isinstance(result, dict) else 'ë”•ì…”ë„ˆë¦¬ ì•„ë‹˜'}")
                
                # ì‘ë‹µì— ëŒ€í•œ ì£¼ìš” ì •ë³´ ë¡œê·¸
                if isinstance(result, dict):
                    if "success" in result:
                        log_step("CallRESTAPI", f"ğŸ¯ API ì„±ê³µ: {result.get('success')}")
                    if "results" in result:
                        log_step("CallRESTAPI", f"ğŸ“‹ ê²°ê³¼ ê°œìˆ˜: {len(result.get('results', []))}")
                    if "total_found" in result:
                        log_step("CallRESTAPI", f"ğŸ” ì´ ë°œê²¬: {result.get('total_found')}")
                    if "error" in result and result.get("error"):
                        log_step("CallRESTAPI", f"âš ï¸ API ì˜¤ë¥˜ ë°˜í™˜: {result.get('error')}")
                
                # ëª…ì‹œì ì¸ ì„±ê³µ í•„ë“œê°€ ì—†ìœ¼ë©´ ì„±ê³µìœ¼ë¡œ í‘œì‹œ
                if "success" not in result:
                    result["success"] = True
                    log_step("CallRESTAPI", f"ğŸ”§ ì‘ë‹µì— success=True ì¶”ê°€")
                
                return result
            except json.JSONDecodeError as e:
                error_msg = f"ì˜ëª»ëœ JSON ì‘ë‹µ: {e}"
                log_step("CallRESTAPI", f"âŒ JSON ë””ì½”ë“œ ì˜¤ë¥˜: {error_msg}")
                log_step("CallRESTAPI", f"ğŸ“„ ì›ì‹œ ì‘ë‹µ: {response.text[:500]}...")
                return {"success": False, "error": error_msg}
        else:
            error_msg = f"HTTP {response.status_code}: {response.text[:200]}..."
            log_step("CallRESTAPI", f"âŒ {service['name']}.{action} ì‹¤íŒ¨: {error_msg}")
            return {"success": False, "error": error_msg}
        
    except Exception as e:
        error_msg = f"ìš”ì²­ ì‹¤íŒ¨: {str(e)}"
        log_step("CallRESTAPI", f"âŒ {service['name']}.{action} ì˜¤ë¥˜: {error_msg}")
        return {"success": False, "error": error_msg}



def main():
    """Main Streamlit application"""
    log_step("MainApp", "Starting main application")

    # Initialize session state
    if "services_config" not in st.session_state:
        st.session_state.services_config = load_services_config()
    if "mcp_discovery" not in st.session_state:
        st.session_state.mcp_discovery = MCPToolDiscovery(st.session_state.services_config)
    if "rest_service_manager" not in st.session_state:
        st.session_state.rest_service_manager = RESTServiceManager(st.session_state.services_config)
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "log_messages" not in st.session_state:
        st.session_state.log_messages = []
    if "service_states" not in st.session_state:
        st.session_state.service_states = {key: service.get('enabled', False) for key, service in st.session_state.services_config.items()}

    st.title("ğŸ”¬ LiveLabs AI Assistant")
    st.markdown("*Powered by REST APIs and OCI Generative AI*")

    with st.sidebar:
        st.header("ğŸ”§ Service Management")

        st.subheader("Service Activation")
        for service_key, service in st.session_state.services_config.items():
            current_state = st.session_state.service_states.get(service_key, False)
            new_state = st.toggle(
                f"{service['name']}",
                value=current_state,
                key=f"toggle_{service_key}",
                help=f"Enable/disable {service['description']}"
            )
            if new_state != current_state:
                st.session_state.service_states[service_key] = new_state
                log_step("ServiceToggle", f"{service['name']} {'enabled' if new_state else 'disabled'}")

                if new_state:  # If service is enabled, discover tools
                    with st.spinner(f"Discovering tools for {service['name']}..."):
                        try:
                            import asyncio
                            tools_data = asyncio.run(st.session_state.mcp_discovery.discover_tools(service_key))
                            st.success(f"âœ… Found {len(tools_data.get('tools', []))} tools for {service['name']}")
                        except Exception as e:
                            st.error(f"âŒ Tool discovery failed for {service['name']}: {str(e)}")
                else: # If service is disabled, clear the cache
                    if 'tools_cache' in st.session_state.services_config[service_key]:
                        del st.session_state.services_config[service_key]['tools_cache']
                        log_step("Cache", f"Cleared tools cache for {service['name']}")

                st.rerun()

        st.subheader("ğŸ“Š Service Status")
        if st.button("ğŸ©º Check All Services", key="check_services"):
            with st.spinner("Checking service health..."):
                st.session_state.health_check_results = st.session_state.rest_service_manager.test_all_services()
                st.rerun()

        if "health_check_results" in st.session_state:
            for service_key, result in st.session_state.health_check_results.items():
                if st.session_state.service_states.get(service_key):
                    status = result['status']
                    if status == 'healthy':
                        st.success(f"âœ… {st.session_state.services_config[service_key]['name']}: Healthy ({result['response_time']}ms)")
                    else:
                        st.warning(f"âš ï¸ {st.session_state.services_config[service_key]['name']}: {result['message']}")

        with st.expander("âš™ï¸ Service Configuration & Tools"):
            st.subheader("Loaded `SERVICES` Configuration")
            st.json(st.session_state.services_config)
            st.subheader("Discovered Tools Cache")
            cached_tools_services = [s for s, d in st.session_state.services_config.items() if d.get("tools_cache")]
            if cached_tools_services:
                for service_key in cached_tools_services:
                    with st.expander(f"ğŸ› ï¸ Tools for {st.session_state.services_config[service_key]['name']}"):
                        st.json(st.session_state.services_config[service_key]["tools_cache"])
            else:
                st.info("No tools cached. Use 'Discover Tools' first.")

        st.subheader("âš¡ Quick Actions")
        enabled_services_for_action = [k for k, v in st.session_state.service_states.items() if v]
        if not enabled_services_for_action:
            st.info("Enable a service to see its actions.")
        else:
            selected_service_for_action = st.selectbox(
                "Select a service",
                options=enabled_services_for_action,
                format_func=lambda x: st.session_state.services_config[x]['name'],
                key="quick_action_service"
            )

            if selected_service_for_action:
                available_tools = st.session_state.mcp_discovery.get_available_tools(selected_service_for_action)
                tool_names = [tool['name'] for tool in available_tools]

                if not tool_names:
                    st.warning("No tools discovered for this service. Use 'Discover Tools' first.")
                else:
                    selected_action = st.selectbox(
                        "Select an action",
                        options=tool_names,
                        key="quick_action_action"
                    )

                    if selected_action:
                        selected_tool = next((t for t in available_tools if t['name'] == selected_action), None)
                        params = {}

                        if selected_tool and 'parameters' in selected_tool:
                            st.markdown("**Parameters**")
                            for param in selected_tool['parameters']:
                                if isinstance(param, dict):
                                    param_name = param.get('name')
                                    param_type = param.get('type', 'string')
                                else:
                                    param_name = param
                                    param_type = 'string'

                                if param_name:
                                    params[param_name] = st.text_input(
                                        f"Parameter: `{param_name}` ({param_type})",
                                        key=f"param_{selected_service_for_action}_{selected_action}_{param_name}"
                                    )

                        if st.button("ğŸš€ Run Action", key="run_quick_action"):
                            with st.spinner(f"Running {selected_action}..."):
                                final_params = {k: v for k, v in params.items() if v}
                                result = call_rest_api(selected_service_for_action, selected_action, final_params)
                                st.session_state.last_action_result = result

        if "last_action_result" in st.session_state:
            with st.expander("Last Action Result", expanded=True):
                st.json(st.session_state.last_action_result)

        st.subheader("ğŸ“ Activity Log")
        if st.button("Clear Log", key="clear_log"):
            st.session_state.log_messages = []
            st.rerun()

        log_container = st.container()
        with log_container:
            for msg in reversed(st.session_state.get("log_messages", [])):
                st.text(msg)

    # Main chat interface
    st.header("ğŸ’¬ Chat with LiveLabs AI")

    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "api_response" in message and message["api_response"]:
                with st.expander("View Full API Response"):
                    st.json(message["api_response"])
            if "ai_analysis" in message and message["ai_analysis"]:
                with st.expander("View AI Reasoning Analysis"):
                    st.json(message["ai_analysis"])

            # Display workflow steps if available
            if "api_response" in message and message["api_response"] and "steps" in message["api_response"]:
                with st.expander("View Workflow Steps"):
                    for step in message["api_response"]["steps"]:
                        st.markdown(f"**Step {step['step']}: {step['service']} -> {step['action']}**")
                        if step['result'].get('success'):
                            st.success("Status: Success")
                        else:
                            st.error("Status: Failed")
                        
                        with st.container():
                            st.markdown("**Parameters:**")
                            st.json(step['parameters'])
                            
                            st.markdown("**Reasoning:**")
                            st.info(step.get('reasoning', 'No reasoning provided.'))

                            st.markdown("**Result:**")
                            st.json(step['result'])

    if prompt := st.chat_input("Ask me about LiveLabs workshops..."):
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            with st.spinner("AI is thinking..."):
                assistant_response, api_response, ai_analysis = generate_chatbot_response_with_data(prompt)
                
                # First, save the complete response to session state
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": assistant_response,
                    "api_response": api_response,
                    "ai_analysis": ai_analysis
                })
                
                # Then, update the UI and force a rerun to show the new message and its details
                message_placeholder.markdown(assistant_response)
                st.rerun()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"An unexpected error occurred: {e}")
        logger.error(f"Unhandled exception in main: {e}", exc_info=True)
